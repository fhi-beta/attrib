---
title: "Introduction to Attrib"
author: "Aurora Hofman"
date: "2020-07-21"
output: rmarkdown::html_vignette
figure_width: 6
figure_height: 4
vignette: >
 %\VignetteIndexEntry{Introduction to Attrib}
 %\VignetteEncoding{UTF-8}
 %\VignetteEngine{knitr::rmarkdown}
editor_options:
chunk_output_type: console
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(attrib)
```

# Introduciton
Attrib provides a way of estimating what the mortality would have been if some given exposures are set to a referance value. By using simulations from the posterior distribution of all coefficiants one gets the opportunity to easily aggregate over time and locations and still be able to optain credible intevals of the desired percentages. 

This example will go throug how to use fit_attrib to fit the data, how to use est_attrib to estimate the mortality given exposures and referance values and some examples of usages of the resulting dataset. 

## Data example

We will use the datasets fake_data_county and fake_data_nation.
Fake_data_county consists of fake date of mortalities for all counties of Norway on a weekly basis from 2010 untill 2020. The dataset consists of the followig features:

* location_code: Location code of the differend counties
* week: Week number
* season: Years of the season
* year: Year
* yrwk: Year and week
* x: Number of weeks from the start of the season
* pop: Population size
* pr100_ili: Percentage of doctorsconsultations diagnosed with influenza like ilnesses 
* pr100_ili_lag_1: pr_100_ili lagged wiht one week
* temperature: Temperature
* temperature_high: number of heatwaves
* deaths: number of mortalities 

Fake_data_nation consists of the same features but contains national information. 


```{r}
data_fake_county <- attrib::data_fake_county
data_fake_nation <- attrib::data_fake_nation
head(data_fake_county, 5)
```

In this example we will look at the exposures pr100_ili_lag_1 and temperature_high and calculate the attributable mortalities due to these exposures. 

# Fitting using fit_attrib

## County level

We want to estimate the attributable mortality due to ILI and heatwaves. Attrib lets us fit models with both fixed and random effect and offsets using linear mixed models. To do so the glmer function from lme4 package is used. This means we must specify which are the offsets, the fixed effects and the random effects. We must also spesify the response. In our case we will model the response  *deaths* as a function of:

* the fixed effects:
  * temperature_high
  * pr100_ili_lag_1
  * sin(2 * pi * (week - 1) / 52) 
  * cos(2 * pi * (week - 1) / 52)
* the random effects:
  * (1|location_code)
  * (pr100_ili_lag_1|season)
* the offset:
  * log(pop)

```{r}
#response
response <- "deaths"

# fixed effects
fixef_county <- " temperature_high +
  pr100_ili_lag_1 +
  sin(2 * pi * (week - 1) / 52) +
  cos(2 * pi * (week - 1) / 52)"


#random effects
ranef_county <- "(1|location_code) +
  (pr100_ili_lag_1|season)"

#offset
offset_county <- "log(pop)"

```

Now we fit the model using fit_attrib. 

```{r, message=FALSE, warning = FALSE}

fit_county <- fit_attrib(data_fake_county, 
                          response = response, 
                          fixef = fixef_county, 
                          ranef = ranef_county, 
                          offset = offset_county)

```

This results in the following fit. 
```{r}
fit_county
```

Note that fit has the added attributes offset, saving the offset name, and fit_fix, the coefficients of the linear model fitted on only the fixed effects. These are needed by est_attrib later on to create the dataset containing only the fixed effects. 


##  National level

We estimate the same as before But on a national level, meaning we remove the random effect (1|location_code) sinse we only have one location code. This gives the following features:

* the fixed effects:
  * temperature_high
  * pr100_ili_lag_1
  * sin(2 * pi * (week - 1) / 52) 
  * cos(2 * pi * (week - 1) / 52)
* the random effects:
  * (pr100_ili_lag_1|season)
* the offset:
  * log(pop)

```{r}
# take in the fixed effects
response = "deaths"
fixef_nation <- "temperature_high +
  pr100_ili_lag_1 +
  sin(2 * pi * (week - 1) / 52) +
  cos(2 * pi * (week - 1) / 52)"


#take in the random effects
ranef_nation <- "(pr100_ili_lag_1|season)"

# take in the offset
offset_nation <- "log(pop)"

```


```{r, message = FALSE, warning=FALSE}

  fit_nation <- fit_attrib(data_fake_nation, 
                           response = response, 
                           fixef = fixef_nation, 
                           ranef = ranef_nation, 
                           offset = offset_nation)

```

# Using the sim function

The sim function can be used to generate simulations for all the rows in our data. It first generater 500 simulations from the posterior distribution of the coefficients from out fit before applying these coefficiants on our dataset generating 500 simululations og expected mortality for each line. This is quite generic. Hence if the goal is to compute attributable mortalities or incident risk ratios we use est_attrib as shown below. 

```{r}
sim_data <- sim(fit_nation, data_fake_nation)
```
```{r}
head(sim_data[id_row == 1], 5)
```

We can see that we now have multiple expected mortalities for the same dataline. This is due to the coefficiant simmulations. 

# Estimating referance mortality using est_attrib

To generate simulations for each observation and estimate the mortality given the referance values of the exposures we use attribs est_attrib. 
We need to give the fit, the dataset and the exposures with referance values. Est_attrib will then using the sim function from the arm pachage to generate simulations of the underlying posterior distribution before using this to compute the estimated mortalities for each simulation. 

```{r}
exposures <- list( "temperature_high" = 0, "pr100_ili_lag_1" = 0)
est_attrib_sim_county <- attrib::est_attrib(fit_county, 
                                        data_fake_county, 
                                        exposures = exposures)
est_attrib_sim_nation <- attrib::est_attrib(fit_nation, 
                                        data_fake_nation, 
                                        exposures = exposures )
```

```{r}
head(est_attrib_sim_county, 5)
```
Now we can see in the above dataset that the colums *id*, *sim_id*,  *exp_mort_observed*, *exp_mort_temperature_high=0*, *exp_mort_pr100_ili_lag_1=0* are added to the previous set of colums. For each row in the original dataset we now have 500 
<!-- change this if we change n_sim --> 
rows, one for each of the simulations done by est_attrib. In each row we see the estimate of the number of mortalities given a refereance value for *exp_mort_temperature_high* and *exp_mort_pr100_ili_lag_1*.

To make the dataprocessing easier later we convert the dataset from wide to long form and collapse the estimated mortalities 
```{r}
est_attrib_county_long<-data.table::melt.data.table(est_attrib_sim_county, 
                                                  id.vars = c("location_code", 
                                                              "season",  
                                                              "x", 
                                                              "week", 
                                                              "id", 
                                                              "sim_id", 
                                                              "deaths", 
                                                              "exp_mort_observed"),
                                           measure.vars = c("exp_mort_temperature_high=0", 
                                                            "exp_mort_pr100_ili_lag_1=0")) 
data.table::setnames(est_attrib_county_long, "variable", "attr")

head(est_attrib_county_long, 5)
```

We can see that the columns *exp_mort_temperature_high=0*, *exp_mort_pr100_ili_lag_1=0* are now collapsed into the new column *attr* and *value* with *attr* discribing which exposure we have and *value* giving the corresponding referance value. 

```{r}
est_attrib_nation_long<-data.table::melt.data.table(est_attrib_sim_nation, 
                                                  id.vars = c("location_code", 
                                                              "season",  
                                                              "x", 
                                                              "week", 
                                                              "id", 
                                                              "sim_id", 
                                                              "deaths", 
                                                              "exp_mort_observed"),
                                           measure.vars = c("exp_mort_temperature_high=0", 
                                                            "exp_mort_pr100_ili_lag_1=0")) 
data.table::setnames(est_attrib_nation_long, "variable", "attr")

```

# Compare the national data to data aggregated from county to national level.
Sinse we now have two datasets, one on a countylevel and one on a national level, to compare them some aggregations is needed. 

## Aggregate from county to national level seasonaly
To aggregate from the the current dataset to a national dataset with seasonal data for *est_attrib_county_long*, the county dataset, we sums *exp_mort_observed*, *value* and *deaths* for all counties and weeks per season. Afterwards we calculate the expected attributable mortality, exp_attr, by substracting *value* (the estimated number of mortalities given the referance value of the exposure) from *the exp_mort_observed* and the incident risk ration, exp_irr, by deviding *exp_mort_observed* by *value*. 
To be able to separate this dataset from the other we add a tag. 
```{r}
aggregated_county_to_nation <-  est_attrib_county_long[,.(
  exp_mort_observed = sum(exp_mort_observed),
  value = sum(value), 
  deaths = sum(deaths)
), keyby = .(season, attr, sim_id)]

# Add exp_attr, exp_irr and a tag.
aggregated_county_to_nation[, exp_attr:= (exp_mort_observed - value)]
aggregated_county_to_nation[, exp_irr:= (exp_mort_observed/value)]
aggregated_county_to_nation[, tag := "aggregated_from_county"]

head(aggregated_county_to_nation, 5)
```

We can see that we no longer have the colums *location_code*, *week* and *x* sinse the data is now aggregated to a national level on a seasonal basis.

## Aggregating the national model per season
For the national model we sum the same feathures but only for all weeks per season and create exp_attr and exp_irr in the same way as above.
```{r}
aggregated_nation <-  est_attrib_nation_long[, .(
  exp_mort_observed = sum(exp_mort_observed),
  value = sum(value), 
  deaths = sum(deaths)
), keyby = .(season, attr, sim_id)]



aggregated_nation[, exp_attr:= (exp_mort_observed - value)]
aggregated_nation[, exp_irr:= (exp_mort_observed/value)]
aggregated_nation[, tag:= "nation"]
head(aggregated_nation, 5)
```
Again we kan see that the data does no longer contain weekly information but is aggregated to a seasonal basis.

For simplicity we rbindlist the two datasets together. 
```{r}
library(ggplot2)
data_national<- data.table::rbindlist(list(aggregated_county_to_nation, aggregated_nation))
```


## Calculate simulation quantiles. 

The next thing to do is to aggregate away the simulations. The benefits of having the simulations is the posibility it gives to efficiently compute al desireable quantiles. For this example we will ue the .05, .5 and .95 quantiles.

```{r}
# Quantile functins
q05 <- function(x){
  return(quantile(x, 0.05))
}
q95 <- function(x){
  return(quantile(x, 0.95))
}
```

We compute the quantiles for *exp_attr* and *exp_irr* in the following way. 
```{r}
col_names <- colnames(data_national)
data.table::setkeyv(data_national, 
                    col_names[!col_names %in% c("exp_attr", 
                                                "exp_irr",
                                                "sim_id", 
                                                "exp_mort_observed", 
                                                "value", 
                                                "deaths")])

aggregated_sim_seasonal_data_national<- data_national[,
                                   unlist(recursive = FALSE, 
                                          lapply(.(median = median, q05 = q05, q95 = q95),
                                                                    function(f) lapply(.SD, f)
                                   )), 
                                   by = eval(data.table::key(data_national)),
                                   .SDcols = c("exp_attr", "exp_irr")]

head(aggregated_sim_seasonal_data_national,5)
```

We can now see that we have confidance intervals and estimates for both the attributable deaths and the incident risk ratio for all exposures. 

## Plot to compare the national with the aggregated county to natoinal model

To be able to compare the two models we make a pointrange plot using ggplot2. 

```{r fig.height=4, fig.width=6}
q <- ggplot(aggregated_sim_seasonal_data_national[attr == "exp_mort_pr100_ili_lag_1=0"], 
                       aes(x = season, y = median.exp_attr, group = tag, color = tag)) 
q <- q + geom_pointrange(aes(x = season, y = median.exp_attr, ymin = q05.exp_attr, ymax = q95.exp_attr), position = position_dodge(width = 0.3))
q <- q + ggtitle("Attributable mortality due to ILI in Norway according to 2 models") 
q <- q +  scale_y_continuous("Estimated attributable mortality") 
q <- q +  theme(axis.text.x = element_text(angle = 90),axis.title.x=element_blank()) 
q <- q +  labs(caption = glue::glue(" Aggregated county model: Attributable mortality modeled on a county level before beeing aggregated up to a national level.\n National model: Attributable mortality modeled on a national level.\n Folkehelseinstituttet 14.07.2020"))
q

```

# Aggregating the county model to compare seasons. 
As a contrast to what we did in the section about comparing the national data to data aggregated from county to national level, we now want to keep the information on a weekly basis. 

Hence the aggregation is done in the same way as before with the exeption of including *x* and *week* in the keyby funciton.
<!-- is keyby a function? -->
```{r}
aggregated_county_to_nation <-  est_attrib_county_long[, .(
  exp_mort_observed = sum(exp_mort_observed),
  value = sum(value), 
  deaths = sum(deaths)
), keyby = .(season, x, week, attr, sim_id)]

aggregated_county_to_nation[, exp_attr:= (exp_mort_observed - value)]
aggregated_county_to_nation[, exp_irr:= (exp_mort_observed/value)]
head(aggregated_county_to_nation,5)
```
We observe that the *location_code* is aggregated away but we still have both *x* and *week*

Again we compute the quantiles. 
```{r}

col_names <- colnames(aggregated_county_to_nation)
data.table::setkeyv(aggregated_county_to_nation, col_names[!col_names %in% c("exp_attr", "exp_irr","sim_id", "exposures", "exp_mort_observed", "value")])

aggregated_county_to_nation_weekly <- aggregated_county_to_nation[,
              unlist(recursive = FALSE, lapply(.(median = median, q05 = q05, q95 = q95),
                                               function(f) lapply(.SD, f)
              )), 
              by=eval(data.table::key(aggregated_county_to_nation)),
              .SDcols = c("exp_attr", "exp_irr")]
```

To be able to visualise the attributable deaths cumulativ we add this feature to the dataset using the cumsum function. 
```{r}
aggregated_county_to_nation_weekly[, cumsum := cumsum(median.exp_attr), by = .( attr, season)]
aggregated_county_to_nation_weekly[, cumsum_q05 := cumsum(q05.exp_attr), by = .( attr, season)]
aggregated_county_to_nation_weekly[, cumsum_q95 := cumsum(q95.exp_attr), by = .( attr, season)]

head(aggregated_county_to_nation_weekly, 5)
```
We now have the quantiles and median as before as well as the cumulativ values on a seasonal basis. 

## Cumulativ plot for attributable mortality due to ILI 

We use ggplot2s geom_line and geom_ribbon to nicely visualize the estimates of attributable mortality for the diferent seasons and adding the ninty percent confidance interval to the last season. 
```{r fig.height=4, fig.width=6}
library(ggplot2)
q <-ggplot(data = aggregated_county_to_nation_weekly[season %in% c("2015/2016", "2016/2017", "2017/2018", "2018/2019","2019/2020") & attr == "exp_mort_pr100_ili_lag_1=0"], aes(x = x, y = cumsum, group = season, color = season, fill = season)) 
q <- q +  geom_line() 
q <- q + geom_ribbon(data = aggregated_county_to_nation_weekly[season %in% c("2019/2020") & attr == "exp_mort_pr100_ili_lag_1=0"],
                     aes(ymin = cumsum_q05, ymax = cumsum_q95), alpha = 0.4, colour = NA)

q <- q + scale_y_continuous("Estimated attributable mortality")
q <- q + ggtitle("Estimated mortality due to ILI in Norway")
q

```

## Plot for attributable mortality due to ILI 
```{r}
q <- ggplot(data = aggregated_county_to_nation_weekly[attr == "exp_mort_pr100_ili_lag_1=0"], 
            aes(x = x, y = cumsum, group = season)) 
q <- q +  geom_line(data = aggregated_county_to_nation_weekly[season != "2019/2020" &attr == "exp_mort_pr100_ili_lag_1=0"], 
            aes(x = x, y = median.exp_attr, group = season), color = "grey")
q <- q +  geom_line(data = aggregated_county_to_nation_weekly[season == "2019/2020" &attr == "exp_mort_pr100_ili_lag_1=0"], 
            aes(x = x, y = median.exp_attr, group = season), color = "blue")
q <- q +  geom_ribbon(data = aggregated_county_to_nation_weekly[season == "2019/2020" &attr == "exp_mort_pr100_ili_lag_1=0"], 
              aes(x = x, ymin = q05.exp_attr, ymax = q95.exp_attr), fill = "blue", alpha=0.4)

q <- q + scale_y_continuous("Estimated attributable mortality")
q <- q + ggtitle("Estimated mortality due to ILI per week")
q
```

# Incident rate ratio

Untill now we have focused moslty on attributable mortalities. Now we will take a look at how to cumpute the incident rate ratio for *pr100_ili_lag_1*. Do do this we will use the fit made by fit_attrib on the county dataset but we will change the values for *pr100_ili_lag_1* to 1.

```{r}
data_fake_county_irr <- data.table::copy(data_fake_county)
data_fake_county_irr[, pr100_ili_lag_1 := 1]
head(data_fake_county_irr, 5)
```

Then we can set the referance value to zero and hence optain the risk ratio for the given exposures. 
```{r}
exposures_irr = c(pr100_ili_lag_1 = 0)
```

Now we use est_attrib to create the simmulations. 
```{r}
est_attrib_sim_county_irr <- attrib::est_attrib(fit_county, 
                                        data_fake_county_irr, 
                                        exposures = exposures_irr)
head(est_attrib_sim_county_irr, 5)
```
We see we have obtained values for the referance of the exposure in the same way as before. The difference now is that we changed the dataset before running *est_attrib* meaning the *exp_mort_observed* will now deviate a from the true number of deaths and be the expected number of deaths given *pr100_ili_lag_1* equall to 1. 

The next step is as before to aggregate from a county, weekly to a national seasonal dataset as well ass adding the values for the incident risk ratio to the column *exp_irr*.

```{r}
aggregated_county_to_nation_sim_irr <-  est_attrib_sim_county_irr[, .(
  exp_mort_observed = sum(exp_mort_observed),
  "exp_mort_pr100_ili_lag_1=0"= sum(`exp_mort_pr100_ili_lag_1=0`), 
  deaths = sum(deaths)
), keyby = .(season, sim_id)]
```

```{r}
aggregated_county_to_nation_sim_irr[, exp_irr:= (exp_mort_observed/`exp_mort_pr100_ili_lag_1=0`
)]
head(aggregated_county_to_nation_sim_irr,5)
```

Now we can compute the quantiles. 
```{r}

col_names <- colnames(aggregated_county_to_nation_sim_irr)
data.table::setkeyv(aggregated_county_to_nation_sim_irr, col_names[!col_names %in% c( "exp_irr","sim_id", "exp_mort_observed", "exp_mort_pr100_ili_lag_1=0")])

aggregated_county_to_nation_irr <- aggregated_county_to_nation_sim_irr[,
              unlist(recursive = FALSE, lapply(.(median = median, q05 = q05, q95 = q95),
                                               function(f) lapply(.SD, f)
              )), 
              by=eval(data.table::key(aggregated_county_to_nation_sim_irr)),
              .SDcols = c("exp_irr")]
aggregated_county_to_nation_irr[, tag := "aggregated"]

aggregated_county_to_nation_irr
```

Now we compare the resulting values for irr with the ones obtained by coef(fit_county)$season and the 90 credible interval computed manually using the standard deviation given by summary(fit_county) for *pr100_ili_lag_1*.

```{r}
coef_fit_county <- data.table::as.data.table(coef(fit_county)$season)
col_names_coef <- c("pr100_ili_lag_1")
coef_irr_data <- coef_fit_county[, ..col_names_coef]
coef_irr_data[, irr := exp(pr100_ili_lag_1)]
coef_irr_data[, q05 := exp(pr100_ili_lag_1 - 1.645 *0.003761)]  # 0.003761 is the standard deviation from coef(fit_county)
coef_irr_data[, q95 := exp(pr100_ili_lag_1 + 1.645 *0.003761)]
coef_irr_data[, tag := "from_coef"]
coef_irr_data
```

Add the correct seasons to the data. 
```{r}
coef_irr_data <- cbind(season = aggregated_county_to_nation_irr$season, coef_irr_data)
coef_irr_data
```

Rbindlist the two datasets together. 
```{r}
total_data_irr <- data.table::rbindlist(list(coef_irr_data, aggregated_county_to_nation_irr), use.names = FALSE)
total_data_irr[, pr100_ili_lag_1 := NULL]
total_data_irr
```

```{r fig.height=4, fig.width=6}
q <- ggplot(data = total_data_irr, 
            aes(x = season, group = tag, color = tag)) 
q <- q +  geom_pointrange(aes(y = irr, ymin = q05, ymax = q95), position = position_dodge(width = 0.3))
q <- q +  theme(axis.text.x = element_text(angle = 90),axis.title.x=element_blank())
q <- q + labs(y = "Incident risk ratio")
q <- q + ggtitle("Incident risk ratio for ILI per season")
q
```

As we can see these intervals are very similar.

The benefit of the aggregated aproach is that this prosess will be equally easy no matter the complexity of what we want to compute the irr for. 
We do not have ot take the variance covariance matrix in to account at any stage or manually compute credible intervals. 


